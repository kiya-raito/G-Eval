---
# Ollama と DeepEval を使った LLM 評価

このノートブックは、**Ollama** と **deepeval** を使用して、大規模言語モデル（LLM）の評価を行うためのものです。

---

## 使用ツール

* **Ollama**: ローカル環境でLLMを実行するためのツールです。
* **deepeval**: LLMの出力品質を評価するためのPythonライブラリです。

---

## 評価の流れ

1.  Google Drive をマウントします。
2.  必要なライブラリ、Ollama、および CUDA ドライバをインストールします。
3.  Ollama サーバーを起動します。
4.  モデル（例: `llama3.2`）をダウンロードします。
5.  モデルを deepeval で利用できるように設定します。
6.  **評価メトリック (criteria)** を定義します。
7.  入力、実出力、および正解を指定して評価を実行します。
8.  結果（スコア、理由）を **TSV形式** で保存します。

---

## 注意点

* **評価基準 (criteria)** を変更することで、LLMの評価方針を自由にカスタマイズできます。
* 入力 (`input`)、実出力 (`actual_output`)、正解 (`expected_output`) は **TSVファイル** で指定します。
* GPUがT4（通常メモリ）でも動作確認済みです。

---

必要に応じて、**評価メトリック** や **入力ファイルの列名** などを編集してお使いください。